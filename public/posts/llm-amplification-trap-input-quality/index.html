<!doctype html><html lang=en dir=auto data-theme=dark><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Confident Ignorance: How LLMs Make You Feel Smart About Being Wrong | AI Survival Blog</title><meta name=keywords content="AI Workflows,LLM,Critical Thinking,Productivity"><meta name=description content="LLMs amplify your current state‚Äîthey don't fix your blind spots. Learn how to avoid the trap of confident ignorance and build a verification framework that works."><meta name=author content="Den Kim"><link rel=canonical href=https://aisurvival.blog/posts/llm-amplification-trap-input-quality/><link crossorigin=anonymous href=/assets/css/stylesheet.d80e3a051274792357d7716d09deab5d9dd3a63d51f693049f1d8393a75d5250.css integrity="sha256-2A46BRJ0eSNX13FtCd6rXZ3Tpj1R9pMEnx2Dk6ddUlA=" rel="preload stylesheet" as=style><link rel=icon href=https://aisurvival.blog/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://aisurvival.blog/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://aisurvival.blog/favicon-32x32.png><link rel=apple-touch-icon href=https://aisurvival.blog/apple-touch-icon.png><link rel=mask-icon href=https://aisurvival.blog/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://aisurvival.blog/posts/llm-amplification-trap-input-quality/><noscript><style>#theme-toggle,.top-link{display:none}</style></noscript><script>localStorage.getItem("pref-theme")==="light"&&(document.querySelector("html").dataset.theme="light")</script><link rel=preconnect href=https://fonts.googleapis.com><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&family=JetBrains+Mono:wght@400;500&display=swap" rel=stylesheet><style>body{font-family:inter,-apple-system,BlinkMacSystemFont,segoe ui,Roboto,Oxygen,Ubuntu,Cantarell,sans-serif}code,pre,.mono{font-family:jetbrains mono,fira code,monospace}.home-info .entry-hint-parent{margin-bottom:2rem}.menu.main-menu a::before{margin-right:.5rem}</style><link rel=icon type=image/x-icon href=/favicon.ico><link rel=apple-touch-icon sizes=180x180 href=/apple-touch-icon.png><script async src="https://www.googletagmanager.com/gtag/js?id=G-MJZGZ1H52C"></script><script>var doNotTrack=!1,dnt=navigator.doNotTrack||window.doNotTrack||navigator.msDoNotTrack,doNotTrack=dnt=="1"||dnt=="yes";if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-MJZGZ1H52C")}</script><meta property="og:url" content="https://aisurvival.blog/posts/llm-amplification-trap-input-quality/"><meta property="og:site_name" content="AI Survival Blog"><meta property="og:title" content="Confident Ignorance: How LLMs Make You Feel Smart About Being Wrong"><meta property="og:description" content="LLMs amplify your current state‚Äîthey don't fix your blind spots. Learn how to avoid the trap of confident ignorance and build a verification framework that works."><meta property="og:locale" content="en-us"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2026-02-24T00:30:00+09:00"><meta property="article:modified_time" content="2026-02-24T00:30:00+09:00"><meta property="article:tag" content="AI Workflows"><meta property="article:tag" content="LLM"><meta property="article:tag" content="Critical Thinking"><meta property="article:tag" content="Productivity"><meta property="og:image" content="https://aisurvival.blog/images/llm-amplify.png"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://aisurvival.blog/images/llm-amplify.png"><meta name=twitter:title content="Confident Ignorance: How LLMs Make You Feel Smart About Being Wrong"><meta name=twitter:description content="LLMs amplify your current state‚Äîthey don't fix your blind spots. Learn how to avoid the trap of confident ignorance and build a verification framework that works."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://aisurvival.blog/posts/"},{"@type":"ListItem","position":2,"name":"Confident Ignorance: How LLMs Make You Feel Smart About Being Wrong","item":"https://aisurvival.blog/posts/llm-amplification-trap-input-quality/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Confident Ignorance: How LLMs Make You Feel Smart About Being Wrong","name":"Confident Ignorance: How LLMs Make You Feel Smart About Being Wrong","description":"LLMs amplify your current state‚Äîthey don't fix your blind spots. Learn how to avoid the trap of confident ignorance and build a verification framework that works.","keywords":["AI Workflows","LLM","Critical Thinking","Productivity"],"articleBody":"üéØ The Trap No One Warns You About You‚Äôve been there. You ask an LLM a question, get a detailed response, and submit it with confidence. Days later, you discover the output was fundamentally flawed‚Äînot because the LLM was wrong, but because your question was framed from a position of ignorance.\nThe uncomfortable truth: LLMs amplify your current state. They don‚Äôt fix it.\n‚ö° The Amplification Problem What ‚ÄúAmplification‚Äù Really Means Your knowledge state ‚Üí LLM ‚Üí Amplified output If you know 30% ‚Üí LLM gives you an impressive-sounding 30% If you know 80% ‚Üí LLM gives you an impressive-sounding 80% The danger: When you don‚Äôt know what you don‚Äôt know, the LLM output feels complete. It‚Äôs confident, detailed, and authoritative. But it‚Äôs authoritative within the frame you set‚Äîand that frame might be completely wrong.\nReal-World Scenarios Scenario What Happens The Risk Financial analysis You ask about P/E ratios. LLM gives detailed analysis. You missed that the company‚Äôs revenue recognition changed. Entire analysis is built on flawed assumptions. Market research You ask about competitors. LLM lists 5 players. You missed the stealth startup that just raised $100M. Technical decision You ask which framework to use. LLM recommends Option A. You didn‚Äôt know Option B exists and is 10x faster. Legal compliance You ask about GDPR requirements. LLM summarizes key points. You missed the industry-specific regulation that applies. üîç Why This Happens: The Structural Problem The Question-Frame Limit Quality of Answer = f(Quality of Question, Domain Knowledge) LLM cannot: - Tell you what you forgot to ask - Know that your premise is wrong - Surface information you don't know to request The Confidence-Ignorance Paradox Before LLMs:\nDon‚Äôt know something ‚Üí ‚ÄúI don‚Äôt know‚Äù ‚Üí You acknowledge the gap After LLMs:\nDon‚Äôt know something ‚Üí Ask LLM ‚Üí Get detailed response ‚Üí Think you know The result: Confident ignorance‚Äîmore dangerous than acknowledged ignorance.\nüõ† Solutions: A Verification Framework Level 1: Expert Review Approach How It Works Best For Domain expert review Have an expert check your draft before submission High-stakes decisions Expert co-drafting Expert provides initial direction, you refine with LLM Complex domains Expert validation LLM draft ‚Üí Expert critique ‚Üí Revision Learning new domains Level 2: Persona Simulation Prompt: \"You are a [domain expert with 20 years experience]. Review this analysis and tell me: 1. What fundamental assumptions might be wrong? 2. What did I miss that a beginner wouldn't know to ask? 3. What would make you reject this analysis?\" Example personas to try:\nSkeptical CFO Academic peer reviewer Contrarian investor Devil‚Äôs advocate Level 3: Multi-LLM Blind Spot Detection LLM Strength Use For GPT-4 Broad knowledge, reasoning Primary analysis Claude Nuanced thinking, safety Ethical considerations, edge cases Perplexity Real-time search, citations Fact verification NotebookLM Document synthesis Deep research from specific sources Workflow:\n1. GPT-4: Generate initial analysis 2. Claude: Challenge assumptions, find blind spots 3. Perplexity: Fact-check key claims 4. NotebookLM: Deep dive into specific areas Level 4: Reverse Questioning Instead of: ‚ÄúIs this analysis correct?‚Äù\nAsk:\n\"What would make this analysis completely wrong?\" \"What scenarios contradict this conclusion?\" \"If this were a bad analysis, how would a critic dismantle it?\" Level 5: Confidence Calibration Prompt: \"Rate your confidence in each claim (0-100%). For claims below 80%, explain what information would increase confidence.\" This forces:\nExplicit uncertainty acknowledgment Identification of evidence gaps Clearer next steps for verification üìã The ‚ÄúWhat I Don‚Äôt Know‚Äù Checklist Before submitting any LLM-assisted work:\n‚ñ° Domain Knowledge Check - Rate my understanding of this domain (0-10) - Can I verify the core claims without LLM help? ‚ñ° Assumption Audit - What assumptions am I making? - What if these assumptions are wrong? ‚ñ° Blind Spot Hunt - What did I not think to ask? - What would an expert ask that I didn't? ‚ñ° Contradiction Search - What evidence would contradict my conclusion? - Have I searched for that evidence? ‚ñ° Source Verification - Can I trace key claims to primary sources? - Are citations real or hallucinated? üéì Practical Workflow: The 4-Pass System Multiple verification passes catch different types of errors.\nPass 1: Generation (Any LLM) Draft your analysis/response Focus on structure and completeness Pass 2: Challenge (Different LLM) Ask another model to find flaws Use reverse questioning prompts Focus on blind spots Pass 3: Fact-Check (Perplexity/NotebookLM) Verify key claims Find primary sources Check for hallucinations Pass 4: Expert Simulation Have LLM role-play domain expert Request harsh critique Revise based on feedback üí° Advanced Techniques Technique 1: The ‚ÄúUnknown Unknowns‚Äù Prompt \"I'm analyzing [topic]. My current understanding covers: - [Point A] - [Point B] - [Point C] What critical aspects am I likely missing? What would a beginner in this domain not know to ask about? What are the 'unknown unknowns' here?\" Technique 2: The Confidence Interval \"For each major claim in this analysis: 1. Assign a confidence score (0-100%) 2. Identify what evidence would change this score 3. Note any claims you cannot verify\" Technique 3: The Temporal Check \"When was the training data cutoff for your knowledge? For claims about [recent event/company/market]: - How certain are you this information is current? - What might have changed since your training?\" Technique 4: The Source Trace \"For each statistic and claim: - What is the original source? - Can you provide a verifiable link? - If uncertain, flag as 'unverified'\" ‚ö†Ô∏è Common Pitfalls Pitfall Why It‚Äôs Dangerous How to Avoid ‚ÄúIt sounds authoritative‚Äù Authority ‚â† accuracy Verify, don‚Äôt trust ‚ÄúIt cited sources‚Äù Citations can be hallucinated Check links manually ‚ÄúMultiple LLMs agreed‚Äù All may share the same blind spots Use fundamentally different approaches ‚ÄúI refined it 5 times‚Äù Refinement within wrong frame = polished wrong answer Challenge the frame, not just details üéØ The Meta-Pattern What separates effective LLM users from ineffective ones:\nFactor Ineffective User Effective User Starting point ‚ÄúLet me ask the LLM‚Äù ‚ÄúLet me understand what I know and don‚Äôt know‚Äù Verification ‚ÄúLooks reasonable‚Äù ‚ÄúHow might this be wrong?‚Äù Confidence High after LLM output Calibrated, with explicit uncertainty Process Single-pass generation Multi-pass with verification loops Expertise Assumes LLM fills gaps Knows LLM amplifies existing knowledge üíé Key Takeaways LLMs amplify‚Äîthey don‚Äôt fill gaps. Your output quality is bounded by your input quality.\nConfidence without verification is dangerous. The most harmful mistakes come from outputs that feel right.\nMultiple passes catch different errors. Generation ‚Üí Challenge ‚Üí Fact-check ‚Üí Expert simulation.\nYour domain knowledge still matters. LLMs are tools for experts, not replacements for expertise.\nAsk what you‚Äôre missing, not just what you want. The ‚Äúunknown unknowns‚Äù prompt is your friend.\nüìö Recommended Reading These books helped shape my thinking on this topic:\nBook Author Why It Helps Get It Thinking, Fast and Slow Daniel Kahneman Understanding cognitive biases and System 1 vs System 2 thinking Amazon The Checklist Manifesto Atul Gawande The power of structured verification in complex tasks Amazon Superforecasting Philip Tetlock Calibration and probabilistic thinking for better predictions Amazon The best LLM users aren‚Äôt those who ask the best questions.\nThey‚Äôre those who know what questions they should have asked‚Äîand build systems to find them.\nAs an Amazon Associate, I earn from qualifying purchases. I only recommend books I‚Äôve actually read and found valuable.\n","wordCount":"1196","inLanguage":"en","image":"https://aisurvival.blog/images/llm-amplify.png","datePublished":"2026-02-24T00:30:00+09:00","dateModified":"2026-02-24T00:30:00+09:00","author":{"@type":"Person","name":"Den Kim"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://aisurvival.blog/posts/llm-amplification-trap-input-quality/"},"publisher":{"@type":"Organization","name":"AI Survival Blog","logo":{"@type":"ImageObject","url":"https://aisurvival.blog/favicon.ico"}}}</script></head><body id=top><header class=header><nav class=nav><div class=logo><a href=https://aisurvival.blog/ accesskey=h title="AI Survival Blog (Alt + H)">AI Survival Blog</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme">
<svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://aisurvival.blog/about/ title=About><span>About</span></a></li><li><a href=https://aisurvival.blog/posts/ title="All Posts"><span>All Posts</span></a></li><li><a href=https://aisurvival.blog/categories/investment-insights/ title="üìä Investment Insights"><span>üìä Investment Insights</span></a></li><li><a href=https://aisurvival.blog/categories/news-curation/ title="üì∞ News Curation"><span>üì∞ News Curation</span></a></li><li><a href=https://aisurvival.blog/categories/ai-workflows/ title="üõ† AI in Workspace"><span>üõ† AI in Workspace</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://aisurvival.blog/>Home</a>&nbsp;¬ª&nbsp;<a href=https://aisurvival.blog/posts/>Posts</a></div><h1 class="post-title entry-hint-parent">Confident Ignorance: How LLMs Make You Feel Smart About Being Wrong</h1><div class=post-description>LLMs amplify your current state‚Äîthey don't fix your blind spots. Learn how to avoid the trap of confident ignorance and build a verification framework that works.</div><div class=post-meta><span title='2026-02-24 00:30:00 +0900 KST'>February 24, 2026</span>&nbsp;¬∑&nbsp;<span>6 min</span>&nbsp;¬∑&nbsp;<span>1196 words</span>&nbsp;¬∑&nbsp;<span>Den Kim</span></div></header><figure class=entry-cover><img loading=eager src=https://aisurvival.blog/images/llm-amplify.png alt="LLM amplification trap"><figcaption>What gets amplified: knowledge or ignorance?</figcaption></figure><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><nav id=TableOfContents><ul><li><a href=#-the-trap-no-one-warns-you-about>üéØ The Trap No One Warns You About</a></li><li><a href=#-the-amplification-problem>‚ö° The Amplification Problem</a><ul><li><a href=#what-amplification-really-means>What &ldquo;Amplification&rdquo; Really Means</a></li><li><a href=#real-world-scenarios>Real-World Scenarios</a></li></ul></li><li><a href=#-why-this-happens-the-structural-problem>üîç Why This Happens: The Structural Problem</a><ul><li><a href=#the-question-frame-limit>The Question-Frame Limit</a></li><li><a href=#the-confidence-ignorance-paradox>The Confidence-Ignorance Paradox</a></li></ul></li><li><a href=#-solutions-a-verification-framework>üõ† Solutions: A Verification Framework</a><ul><li><a href=#level-1-expert-review>Level 1: Expert Review</a></li><li><a href=#level-2-persona-simulation>Level 2: Persona Simulation</a></li><li><a href=#level-3-multi-llm-blind-spot-detection>Level 3: Multi-LLM Blind Spot Detection</a></li><li><a href=#level-4-reverse-questioning>Level 4: Reverse Questioning</a></li><li><a href=#level-5-confidence-calibration>Level 5: Confidence Calibration</a></li></ul></li><li><a href=#-the-what-i-dont-know-checklist>üìã The &ldquo;What I Don&rsquo;t Know&rdquo; Checklist</a></li><li><a href=#-practical-workflow-the-4-pass-system>üéì Practical Workflow: The 4-Pass System</a><ul><li><a href=#pass-1-generation-any-llm>Pass 1: Generation (Any LLM)</a></li><li><a href=#pass-2-challenge-different-llm>Pass 2: Challenge (Different LLM)</a></li><li><a href=#pass-3-fact-check-perplexitynotebooklm>Pass 3: Fact-Check (Perplexity/NotebookLM)</a></li><li><a href=#pass-4-expert-simulation>Pass 4: Expert Simulation</a></li></ul></li><li><a href=#-advanced-techniques>üí° Advanced Techniques</a><ul><li><a href=#technique-1-the-unknown-unknowns-prompt>Technique 1: The &ldquo;Unknown Unknowns&rdquo; Prompt</a></li><li><a href=#technique-2-the-confidence-interval>Technique 2: The Confidence Interval</a></li><li><a href=#technique-3-the-temporal-check>Technique 3: The Temporal Check</a></li><li><a href=#technique-4-the-source-trace>Technique 4: The Source Trace</a></li></ul></li><li><a href=#-common-pitfalls>‚ö†Ô∏è Common Pitfalls</a></li><li><a href=#-the-meta-pattern>üéØ The Meta-Pattern</a></li><li><a href=#-key-takeaways>üíé Key Takeaways</a></li><li><a href=#-recommended-reading>üìö Recommended Reading</a></li></ul></nav></div></details></div><div class=post-content><h2 id=-the-trap-no-one-warns-you-about>üéØ The Trap No One Warns You About<a hidden class=anchor aria-hidden=true href=#-the-trap-no-one-warns-you-about>#</a></h2><p>You&rsquo;ve been there. You ask an LLM a question, get a detailed response, and submit it with confidence. Days later, you discover the output was fundamentally flawed‚Äînot because the LLM was wrong, but because <strong>your question was framed from a position of ignorance.</strong></p><p>The uncomfortable truth: <strong>LLMs amplify your current state. They don&rsquo;t fix it.</strong></p><hr><h2 id=-the-amplification-problem>‚ö° The Amplification Problem<a hidden class=anchor aria-hidden=true href=#-the-amplification-problem>#</a></h2><h3 id=what-amplification-really-means>What &ldquo;Amplification&rdquo; Really Means<a hidden class=anchor aria-hidden=true href=#what-amplification-really-means>#</a></h3><pre tabindex=0><code>Your knowledge state ‚Üí LLM ‚Üí Amplified output

If you know 30% ‚Üí LLM gives you an impressive-sounding 30%
If you know 80% ‚Üí LLM gives you an impressive-sounding 80%
</code></pre><p><strong>The danger:</strong> When you don&rsquo;t know what you don&rsquo;t know, the LLM output <em>feels</em> complete. It&rsquo;s confident, detailed, and authoritative. But it&rsquo;s authoritative within the frame you set‚Äîand that frame might be completely wrong.</p><h3 id=real-world-scenarios>Real-World Scenarios<a hidden class=anchor aria-hidden=true href=#real-world-scenarios>#</a></h3><table><thead><tr><th>Scenario</th><th>What Happens</th><th>The Risk</th></tr></thead><tbody><tr><td><strong>Financial analysis</strong></td><td>You ask about P/E ratios. LLM gives detailed analysis.</td><td>You missed that the company&rsquo;s revenue recognition changed. Entire analysis is built on flawed assumptions.</td></tr><tr><td><strong>Market research</strong></td><td>You ask about competitors. LLM lists 5 players.</td><td>You missed the stealth startup that just raised $100M.</td></tr><tr><td><strong>Technical decision</strong></td><td>You ask which framework to use. LLM recommends Option A.</td><td>You didn&rsquo;t know Option B exists and is 10x faster.</td></tr><tr><td><strong>Legal compliance</strong></td><td>You ask about GDPR requirements. LLM summarizes key points.</td><td>You missed the industry-specific regulation that applies.</td></tr></tbody></table><hr><h2 id=-why-this-happens-the-structural-problem>üîç Why This Happens: The Structural Problem<a hidden class=anchor aria-hidden=true href=#-why-this-happens-the-structural-problem>#</a></h2><h3 id=the-question-frame-limit>The Question-Frame Limit<a hidden class=anchor aria-hidden=true href=#the-question-frame-limit>#</a></h3><pre tabindex=0><code>Quality of Answer = f(Quality of Question, Domain Knowledge)

LLM cannot:
- Tell you what you forgot to ask
- Know that your premise is wrong
- Surface information you don&#39;t know to request
</code></pre><h3 id=the-confidence-ignorance-paradox>The Confidence-Ignorance Paradox<a hidden class=anchor aria-hidden=true href=#the-confidence-ignorance-paradox>#</a></h3><p><strong>Before LLMs:</strong></p><ul><li>Don&rsquo;t know something ‚Üí &ldquo;I don&rsquo;t know&rdquo; ‚Üí You acknowledge the gap</li></ul><p><strong>After LLMs:</strong></p><ul><li>Don&rsquo;t know something ‚Üí Ask LLM ‚Üí Get detailed response ‚Üí <strong>Think you know</strong></li></ul><p>The result: <strong>Confident ignorance</strong>‚Äîmore dangerous than acknowledged ignorance.</p><hr><h2 id=-solutions-a-verification-framework>üõ† Solutions: A Verification Framework<a hidden class=anchor aria-hidden=true href=#-solutions-a-verification-framework>#</a></h2><h3 id=level-1-expert-review>Level 1: Expert Review<a hidden class=anchor aria-hidden=true href=#level-1-expert-review>#</a></h3><table><thead><tr><th>Approach</th><th>How It Works</th><th>Best For</th></tr></thead><tbody><tr><td><strong>Domain expert review</strong></td><td>Have an expert check your draft before submission</td><td>High-stakes decisions</td></tr><tr><td><strong>Expert co-drafting</strong></td><td>Expert provides initial direction, you refine with LLM</td><td>Complex domains</td></tr><tr><td><strong>Expert validation</strong></td><td>LLM draft ‚Üí Expert critique ‚Üí Revision</td><td>Learning new domains</td></tr></tbody></table><h3 id=level-2-persona-simulation>Level 2: Persona Simulation<a hidden class=anchor aria-hidden=true href=#level-2-persona-simulation>#</a></h3><pre tabindex=0><code>Prompt: &#34;You are a [domain expert with 20 years experience]. 
Review this analysis and tell me:
1. What fundamental assumptions might be wrong?
2. What did I miss that a beginner wouldn&#39;t know to ask?
3. What would make you reject this analysis?&#34;
</code></pre><p><strong>Example personas to try:</strong></p><ul><li>Skeptical CFO</li><li>Academic peer reviewer</li><li>Contrarian investor</li><li>Devil&rsquo;s advocate</li></ul><h3 id=level-3-multi-llm-blind-spot-detection>Level 3: Multi-LLM Blind Spot Detection<a hidden class=anchor aria-hidden=true href=#level-3-multi-llm-blind-spot-detection>#</a></h3><table><thead><tr><th>LLM</th><th>Strength</th><th>Use For</th></tr></thead><tbody><tr><td><strong>GPT-4</strong></td><td>Broad knowledge, reasoning</td><td>Primary analysis</td></tr><tr><td><strong>Claude</strong></td><td>Nuanced thinking, safety</td><td>Ethical considerations, edge cases</td></tr><tr><td><strong>Perplexity</strong></td><td>Real-time search, citations</td><td>Fact verification</td></tr><tr><td><strong>NotebookLM</strong></td><td>Document synthesis</td><td>Deep research from specific sources</td></tr></tbody></table><p><strong>Workflow:</strong></p><pre tabindex=0><code>1. GPT-4: Generate initial analysis
2. Claude: Challenge assumptions, find blind spots
3. Perplexity: Fact-check key claims
4. NotebookLM: Deep dive into specific areas
</code></pre><h3 id=level-4-reverse-questioning>Level 4: Reverse Questioning<a hidden class=anchor aria-hidden=true href=#level-4-reverse-questioning>#</a></h3><p><strong>Instead of:</strong> &ldquo;Is this analysis correct?&rdquo;</p><p><strong>Ask:</strong></p><pre tabindex=0><code>&#34;What would make this analysis completely wrong?&#34;
&#34;What scenarios contradict this conclusion?&#34;
&#34;If this were a bad analysis, how would a critic dismantle it?&#34;
</code></pre><h3 id=level-5-confidence-calibration>Level 5: Confidence Calibration<a hidden class=anchor aria-hidden=true href=#level-5-confidence-calibration>#</a></h3><pre tabindex=0><code>Prompt: &#34;Rate your confidence in each claim (0-100%).
For claims below 80%, explain what information would increase confidence.&#34;
</code></pre><p>This forces:</p><ul><li>Explicit uncertainty acknowledgment</li><li>Identification of evidence gaps</li><li>Clearer next steps for verification</li></ul><hr><h2 id=-the-what-i-dont-know-checklist>üìã The &ldquo;What I Don&rsquo;t Know&rdquo; Checklist<a hidden class=anchor aria-hidden=true href=#-the-what-i-dont-know-checklist>#</a></h2><p>Before submitting any LLM-assisted work:</p><pre tabindex=0><code>‚ñ° Domain Knowledge Check
  - Rate my understanding of this domain (0-10)
  - Can I verify the core claims without LLM help?
  
‚ñ° Assumption Audit
  - What assumptions am I making?
  - What if these assumptions are wrong?
  
‚ñ° Blind Spot Hunt
  - What did I not think to ask?
  - What would an expert ask that I didn&#39;t?
  
‚ñ° Contradiction Search
  - What evidence would contradict my conclusion?
  - Have I searched for that evidence?
  
‚ñ° Source Verification
  - Can I trace key claims to primary sources?
  - Are citations real or hallucinated?
</code></pre><hr><h2 id=-practical-workflow-the-4-pass-system>üéì Practical Workflow: The 4-Pass System<a hidden class=anchor aria-hidden=true href=#-practical-workflow-the-4-pass-system>#</a></h2><p><img alt="AI Verification Loop" loading=lazy src=/images/ai-verification-loop.png>
<em>Multiple verification passes catch different types of errors.</em></p><h3 id=pass-1-generation-any-llm>Pass 1: Generation (Any LLM)<a hidden class=anchor aria-hidden=true href=#pass-1-generation-any-llm>#</a></h3><ul><li>Draft your analysis/response</li><li>Focus on structure and completeness</li></ul><h3 id=pass-2-challenge-different-llm>Pass 2: Challenge (Different LLM)<a hidden class=anchor aria-hidden=true href=#pass-2-challenge-different-llm>#</a></h3><ul><li>Ask another model to find flaws</li><li>Use reverse questioning prompts</li><li>Focus on blind spots</li></ul><h3 id=pass-3-fact-check-perplexitynotebooklm>Pass 3: Fact-Check (Perplexity/NotebookLM)<a hidden class=anchor aria-hidden=true href=#pass-3-fact-check-perplexitynotebooklm>#</a></h3><ul><li>Verify key claims</li><li>Find primary sources</li><li>Check for hallucinations</li></ul><h3 id=pass-4-expert-simulation>Pass 4: Expert Simulation<a hidden class=anchor aria-hidden=true href=#pass-4-expert-simulation>#</a></h3><ul><li>Have LLM role-play domain expert</li><li>Request harsh critique</li><li>Revise based on feedback</li></ul><hr><h2 id=-advanced-techniques>üí° Advanced Techniques<a hidden class=anchor aria-hidden=true href=#-advanced-techniques>#</a></h2><h3 id=technique-1-the-unknown-unknowns-prompt>Technique 1: The &ldquo;Unknown Unknowns&rdquo; Prompt<a hidden class=anchor aria-hidden=true href=#technique-1-the-unknown-unknowns-prompt>#</a></h3><pre tabindex=0><code>&#34;I&#39;m analyzing [topic]. My current understanding covers:
- [Point A]
- [Point B]
- [Point C]

What critical aspects am I likely missing?
What would a beginner in this domain not know to ask about?
What are the &#39;unknown unknowns&#39; here?&#34;
</code></pre><h3 id=technique-2-the-confidence-interval>Technique 2: The Confidence Interval<a hidden class=anchor aria-hidden=true href=#technique-2-the-confidence-interval>#</a></h3><pre tabindex=0><code>&#34;For each major claim in this analysis:
1. Assign a confidence score (0-100%)
2. Identify what evidence would change this score
3. Note any claims you cannot verify&#34;
</code></pre><h3 id=technique-3-the-temporal-check>Technique 3: The Temporal Check<a hidden class=anchor aria-hidden=true href=#technique-3-the-temporal-check>#</a></h3><pre tabindex=0><code>&#34;When was the training data cutoff for your knowledge?
For claims about [recent event/company/market]:
- How certain are you this information is current?
- What might have changed since your training?&#34;
</code></pre><h3 id=technique-4-the-source-trace>Technique 4: The Source Trace<a hidden class=anchor aria-hidden=true href=#technique-4-the-source-trace>#</a></h3><pre tabindex=0><code>&#34;For each statistic and claim:
- What is the original source?
- Can you provide a verifiable link?
- If uncertain, flag as &#39;unverified&#39;&#34;
</code></pre><hr><h2 id=-common-pitfalls>‚ö†Ô∏è Common Pitfalls<a hidden class=anchor aria-hidden=true href=#-common-pitfalls>#</a></h2><table><thead><tr><th>Pitfall</th><th>Why It&rsquo;s Dangerous</th><th>How to Avoid</th></tr></thead><tbody><tr><td><strong>&ldquo;It sounds authoritative&rdquo;</strong></td><td>Authority ‚â† accuracy</td><td>Verify, don&rsquo;t trust</td></tr><tr><td><strong>&ldquo;It cited sources&rdquo;</strong></td><td>Citations can be hallucinated</td><td>Check links manually</td></tr><tr><td><strong>&ldquo;Multiple LLMs agreed&rdquo;</strong></td><td>All may share the same blind spots</td><td>Use fundamentally different approaches</td></tr><tr><td><strong>&ldquo;I refined it 5 times&rdquo;</strong></td><td>Refinement within wrong frame = polished wrong answer</td><td>Challenge the frame, not just details</td></tr></tbody></table><hr><h2 id=-the-meta-pattern>üéØ The Meta-Pattern<a hidden class=anchor aria-hidden=true href=#-the-meta-pattern>#</a></h2><p><strong>What separates effective LLM users from ineffective ones:</strong></p><table><thead><tr><th>Factor</th><th>Ineffective User</th><th>Effective User</th></tr></thead><tbody><tr><td><strong>Starting point</strong></td><td>&ldquo;Let me ask the LLM&rdquo;</td><td>&ldquo;Let me understand what I know and don&rsquo;t know&rdquo;</td></tr><tr><td><strong>Verification</strong></td><td>&ldquo;Looks reasonable&rdquo;</td><td>&ldquo;How might this be wrong?&rdquo;</td></tr><tr><td><strong>Confidence</strong></td><td>High after LLM output</td><td>Calibrated, with explicit uncertainty</td></tr><tr><td><strong>Process</strong></td><td>Single-pass generation</td><td>Multi-pass with verification loops</td></tr><tr><td><strong>Expertise</strong></td><td>Assumes LLM fills gaps</td><td>Knows LLM amplifies existing knowledge</td></tr></tbody></table><hr><h2 id=-key-takeaways>üíé Key Takeaways<a hidden class=anchor aria-hidden=true href=#-key-takeaways>#</a></h2><ol><li><p><strong>LLMs amplify‚Äîthey don&rsquo;t fill gaps.</strong> Your output quality is bounded by your input quality.</p></li><li><p><strong>Confidence without verification is dangerous.</strong> The most harmful mistakes come from outputs that <em>feel</em> right.</p></li><li><p><strong>Multiple passes catch different errors.</strong> Generation ‚Üí Challenge ‚Üí Fact-check ‚Üí Expert simulation.</p></li><li><p><strong>Your domain knowledge still matters.</strong> LLMs are tools for experts, not replacements for expertise.</p></li><li><p><strong>Ask what you&rsquo;re missing, not just what you want.</strong> The &ldquo;unknown unknowns&rdquo; prompt is your friend.</p></li></ol><hr><h2 id=-recommended-reading>üìö Recommended Reading<a hidden class=anchor aria-hidden=true href=#-recommended-reading>#</a></h2><p>These books helped shape my thinking on this topic:</p><table><thead><tr><th>Book</th><th>Author</th><th>Why It Helps</th><th>Get It</th></tr></thead><tbody><tr><td><strong>Thinking, Fast and Slow</strong></td><td>Daniel Kahneman</td><td>Understanding cognitive biases and System 1 vs System 2 thinking</td><td><a href="https://www.amazon.com/dp/0374533555?tag=aisurvival-20">Amazon</a></td></tr><tr><td><strong>The Checklist Manifesto</strong></td><td>Atul Gawande</td><td>The power of structured verification in complex tasks</td><td><a href="https://www.amazon.com/dp/0312430000?tag=aisurvival-20">Amazon</a></td></tr><tr><td><strong>Superforecasting</strong></td><td>Philip Tetlock</td><td>Calibration and probabilistic thinking for better predictions</td><td><a href="https://www.amazon.com/dp/0804786618?tag=aisurvival-20">Amazon</a></td></tr></tbody></table><hr><blockquote><p>The best LLM users aren&rsquo;t those who ask the best questions.<br>They&rsquo;re those who know what questions they <em>should have asked</em>‚Äîand build systems to find them.</p></blockquote><hr><p><em>As an Amazon Associate, I earn from qualifying purchases. I only recommend books I&rsquo;ve actually read and found valuable.</em></p></div><footer class=post-footer><ul class=post-tags><li><a href=https://aisurvival.blog/tags/ai-workflows/>AI Workflows</a></li><li><a href=https://aisurvival.blog/tags/llm/>LLM</a></li><li><a href=https://aisurvival.blog/tags/critical-thinking/>Critical Thinking</a></li><li><a href=https://aisurvival.blog/tags/productivity/>Productivity</a></li></ul><nav class=paginav><a class=prev href=https://aisurvival.blog/posts/news/2026-02-25-energy-infrastructure-news/><span class=title>¬´ Prev</span><br><span>Energy & Infrastructure News - Week 8, 2026</span>
</a><a class=next href=https://aisurvival.blog/posts/service-industry-paradox-digital-becomes-air/><span class=title>Next ¬ª</span><br><span>The Service Industry Paradox: When Digital Becomes Air</span></a></nav><ul class=share-buttons><li><a target=_blank rel="noopener noreferrer" aria-label="share Confident Ignorance: How LLMs Make You Feel Smart About Being Wrong on x" href="https://x.com/intent/tweet/?text=Confident%20Ignorance%3a%20How%20LLMs%20Make%20You%20Feel%20Smart%20About%20Being%20Wrong&amp;url=https%3a%2f%2faisurvival.blog%2fposts%2fllm-amplification-trap-input-quality%2f&amp;hashtags=AIWorkflows%2cLLM%2cCriticalThinking%2cProductivity"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446C483.971.0 512 28.03 512 62.554zM269.951 190.75 182.567 75.216H56L207.216 272.95 63.9 436.783h61.366L235.9 310.383l96.667 126.4H456L298.367 228.367l134-153.151H371.033zM127.633 110h36.468l219.38 290.065H349.5z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Confident Ignorance: How LLMs Make You Feel Smart About Being Wrong on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2faisurvival.blog%2fposts%2fllm-amplification-trap-input-quality%2f&amp;title=Confident%20Ignorance%3a%20How%20LLMs%20Make%20You%20Feel%20Smart%20About%20Being%20Wrong&amp;summary=Confident%20Ignorance%3a%20How%20LLMs%20Make%20You%20Feel%20Smart%20About%20Being%20Wrong&amp;source=https%3a%2f%2faisurvival.blog%2fposts%2fllm-amplification-trap-input-quality%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Confident Ignorance: How LLMs Make You Feel Smart About Being Wrong on reddit" href="https://reddit.com/submit?url=https%3a%2f%2faisurvival.blog%2fposts%2fllm-amplification-trap-input-quality%2f&title=Confident%20Ignorance%3a%20How%20LLMs%20Make%20You%20Feel%20Smart%20About%20Being%20Wrong"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Confident Ignorance: How LLMs Make You Feel Smart About Being Wrong on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2faisurvival.blog%2fposts%2fllm-amplification-trap-input-quality%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Confident Ignorance: How LLMs Make You Feel Smart About Being Wrong on whatsapp" href="https://api.whatsapp.com/send?text=Confident%20Ignorance%3a%20How%20LLMs%20Make%20You%20Feel%20Smart%20About%20Being%20Wrong%20-%20https%3a%2f%2faisurvival.blog%2fposts%2fllm-amplification-trap-input-quality%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Confident Ignorance: How LLMs Make You Feel Smart About Being Wrong on telegram" href="https://telegram.me/share/url?text=Confident%20Ignorance%3a%20How%20LLMs%20Make%20You%20Feel%20Smart%20About%20Being%20Wrong&amp;url=https%3a%2f%2faisurvival.blog%2fposts%2fllm-amplification-trap-input-quality%2f"><svg viewBox="2 2 28 28" height="30" width="30" fill="currentColor"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Confident Ignorance: How LLMs Make You Feel Smart About Being Wrong on ycombinator" href="https://news.ycombinator.com/submitlink?t=Confident%20Ignorance%3a%20How%20LLMs%20Make%20You%20Feel%20Smart%20About%20Being%20Wrong&u=https%3a%2f%2faisurvival.blog%2fposts%2fllm-amplification-trap-input-quality%2f"><svg width="30" height="30" viewBox="0 0 512 512" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446zM183.8767 87.9921h-62.034L230.6673 292.4508V424.0079h50.6655V292.4508L390.1575 87.9921H328.1233L256 238.2489z"/></svg></a></li></ul></footer></article></main><footer class=footer><span>&copy; 2026 <a href=https://aisurvival.blog/>AI Survival Blog</a></span> ¬∑
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
<a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");if(menu){const e=localStorage.getItem("menu-scroll-position");e&&(menu.scrollLeft=parseInt(e,10)),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}}document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{const e=document.querySelector("html");e.dataset.theme==="dark"?(e.dataset.theme="light",localStorage.setItem("pref-theme","light")):(e.dataset.theme="dark",localStorage.setItem("pref-theme","dark"))})</script></body></html>